{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"31005_machine_learning_assessment_2.ipynb","provenance":[],"authorship_tag":"ABX9TyOKNgTd91eE/fMxuMrf44sJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VBGzSDMHnmp9"},"source":["# **31005: Machine Learning**\n","\n","## **Assessment 2**\n","\n","#### Summary\n","In this notebook, we implement a decision tree algorithm from scratch. The choice of algorithm is the **Classification and Regression Tree (CART)** algorithm. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node. The algorithm will be trained on the **IRIS dataset**.\n"]},{"cell_type":"markdown","metadata":{"id":"zGJZjerXqwsU"},"source":["### **Import required packages**"]},{"cell_type":"code","metadata":{"id":"-0-eoFICqLy5","executionInfo":{"status":"ok","timestamp":1601960013731,"user_tz":-660,"elapsed":1423,"user":{"displayName":"Patrick Songco","photoUrl":"","userId":"05048570303560894396"}}},"source":["import numpy as np\n","import seaborn as sns\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn import tree as sktree"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mQighrJsrILz"},"source":["### **Create CART_decision_tree Class**\n","\n"]},{"cell_type":"code","metadata":{"id":"kp7SRz9Rqj4r","executionInfo":{"status":"ok","timestamp":1601960019626,"user_tz":-660,"elapsed":1171,"user":{"displayName":"Patrick Songco","photoUrl":"","userId":"05048570303560894396"}}},"source":["class CART_decision_tree(object):\n","    \"\"\"\n","    A recursively defined data structure to store a tree.\n","    Each node can contain other nodes as its children\n","    \"\"\"\n","    def __init__(self, tree = 'cls', criterion = 'gini', prune = 'depth', max_depth = 4, min_criterion = 0.05):\n","        # \"\"\"\"\n","        # The function grows the tree by splitting the data into two branches based on some threshold\n","        # value. The current node is assigned the feature that provides the best split with the corresponding\n","        # threshold value.\n","\n","        # Parameters:\n","        # features (dataframe): [n * p] n observed data samples of p attributes\n","        # target (series): [n] target values\n","        # criterion (string): Type of impurity criterion measure. default = 'gini'\n","        # \"\"\"\n","        self.feature = None\n","        self.featurename = None\n","        self.label = None\n","        self.n_samples = None\n","        self.gain = None\n","        self.left = None\n","        self.right = None\n","        self.threshold = None\n","        self.depth = 0\n","\n","        self.root = None\n","        self.criterion = criterion\n","        self.prune = prune\n","        self.max_depth = max_depth\n","        self.min_criterion = min_criterion\n","        self.tree = tree\n","\n","    def fit(self, features, target):\n","        \"\"\"\n","        The function accepts a training dataset, from which it builds the tree \n","        structure to make decisions or to make children nodes (tree branches) \n","        to do further inquiries.\n","\n","        Parameters:\n","        features (dataframe): [n * p] n observed data samples of p attributes\n","        target (series): [n] target values\n","        \"\"\"\n","        self.root = CART_decision_tree()\n","        if(self.tree == 'cls'):\n","            self.root._grow_tree(features, target, self.criterion)\n","        else:\n","            self.root._grow_tree(features, target, 'mse')\n","        self.root._prune(self.prune, self.max_depth, self.min_criterion, self.root.n_samples)\n","\n","    def predict(self, features):\n","        \"\"\"\n","        The function accepts a test dataset, from which it traverses the CART tree to output an\n","        array of predictions.\n","\n","        Parameters:\n","        features (dataframe): [n * p] n observed data samples of p attributes\n","\n","        Returns:\n","        list: [n] predicted values\n","        \"\"\"                                                                                                   \n","        return [self.root._predict(features.iloc[f,:]) for f in range(features.shape[0])]                           #iterates over each test example and recusively invokes the internal method \"_predicts\" to check\n","                                                                                                                    #if the current node attribute is below or above the threshold value until a prediction can be made.\n","    def print_tree(self):\n","        \"\"\"\n","        Helper function to print tree decision structure.\n","        \"\"\"\n","        self.root._show_tree(0, '')\n","\n","    def _grow_tree(self, features, target, criterion = 'gini'):\n","        \"\"\"\"\n","        The internal function grows the tree by splitting the data into two branches based on some threshold\n","        value. The current node is assigned the feature that provides the best split with the corresponding\n","        threshold value.\n","\n","        Parameters:\n","        features (dataframe): [n * p] n observed data samples of p attributes\n","        target (series): [n] target values\n","        criterion (string): Type of impurity criterion measure. default = 'gini'\n","        \"\"\"\n","        self.n_samples = features.shape[0]\n","        \n","        if len(pd.unique(target)) == 1:                                                                             #if all classes are the same, node is labelled as the class and returned. Terminates recrsion.\n","            self.label = target.iloc[0]\n","            return\n","\n","        best_gain = 0.0\n","        best_feature = None\n","        best_threshold = None\n","\n","        if criterion in {'gini', 'entropy'}:\n","            self.label = max([(c, len(target[target == c])) for c in np.unique(target)], key = lambda x : x[1])[0]  #calculates which class has the most instances in the data and assigns node with that label\n","        else:\n","            self.label = np.mean(target)                                                                            #calculates the mean in a regression problem and assigns node with that label\n","\n","        impurity_node = self._calc_impurity(criterion, target)                                                      #calculates impurity criterion of the parent node\n","        \n","        for col in range(features.shape[1]):                                                                        #iterates over all split decisions to determine the best information gain, feature and threshold\n","            feature_level = pd.unique(features.iloc[:,col])\n","            thresholds = (feature_level[:-1] + feature_level[1:]) / 2.0                                             #create a list of candidate thresholds\n","\n","            for threshold in thresholds:\n","                target_l = target[features.iloc[:,col] <= threshold]                                                #calculates dataset for left child node\n","                impurity_l = self._calc_impurity(criterion, target_l)                                               #calculates left child node impurity criterion\n","                n_l = float(target_l.shape[0]) / self.n_samples                                                     #calculates ratio of samples/total samples \n","\n","                target_r = target[features.iloc[:,col] > threshold]                                                 #calculates dataset for right child node\n","                impurity_r = self._calc_impurity(criterion, target_r)                                               #calculates right child node impurity criterion \n","                n_r = float(target_r.shape[0]) / self.n_samples                                                     #calculates ratio of samples/total samples \n","\n","                information_gain = impurity_node - (n_l * impurity_l + n_r * impurity_r)                            #calculates information gain\n","                if information_gain > best_gain:\n","                    best_gain = information_gain\n","                    best_feature = col\n","                    best_threshold = threshold\n","\n","        self.feature = best_feature\n","        self.featurename = features.columns[best_feature]\n","        self.gain = best_gain\n","        self.threshold = best_threshold\n","        self._split_tree(features, target, criterion)                                                               #iteratively splits the tree and creates children nodes based on spilt criterion\n","\n","    def _split_tree(self, features, target, criterion):\n","        \"\"\"\"\n","        The internal function splits the dataset, creates left and right child nodes and recursively grows the tree.\n","\n","        Parameters:\n","        features (dataframe): [n * p] n observed data samples of p attributes\n","        target (series): [n] target values\n","        criterion (string): Type of impurity criterion measure. default = 'gini'\n","        \"\"\"\n","        features_l = features[features.iloc[:, self.feature] <= self.threshold]                                     #left child node\n","        target_l = target[features.iloc[:, self.feature] <= self.threshold]\n","        self.left = CART_decision_tree()\n","        self.left.depth = self.depth + 1\n","        self.left._grow_tree(features_l, target_l, criterion)\n","\n","        features_r = features[features.iloc[:, self.feature] > self.threshold]                                      #right child node\n","        target_r = target[features.iloc[:, self.feature] > self.threshold]\n","        self.right = CART_decision_tree()\n","        self.right.depth = self.depth + 1\n","        self.right._grow_tree(features_r, target_r, criterion)\n","\n","    def _calc_impurity(self, criterion, target):\n","        \"\"\"\"\n","        The internal function calculates the impurity criterion depending on chosen measure and returns the value.\n","\n","        Parameters:\n","        criterion (string): Type of impurity criterion measure\n","        target (series): [n] target values\n","        \n","        Returns:\n","        float: Impurity criterion\n","        \"\"\"\n","        if criterion == 'gini':\n","            return 1.0 - sum([(float(len(target[target == c])) / float(target.shape[0])) ** 2.0 for c in np.unique(target)])\n","        elif criterion == 'mse':\n","            return np.mean((target - np.mean(target)) ** 2.0)\n","        else:\n","            entropy = 0.0\n","            for c in np.unique(target):\n","                p = float(len(target[target == c])) / target.shape[0]\n","                if p > 0.0:\n","                    entropy -= p * np.log2(p)\n","            return entropy            \n","\n","    def _prune(self, method, max_depth, min_criterion, n_samples):\n","        \"\"\"\"\n","        The internal function tunes the depth of the tree depending on parameters. Reduces overfitting of the data.\n","\n","        Parameters:\n","        method (string): Method of pruning. default = 'depth'\n","        max_depth (int): Maximum depth of the tree\n","        min_criterion (float): Minimum criterion value\n","        n_samples (int): Number of samples in the current node\n","        \"\"\"\n","        if self.feature is None:\n","            return\n","\n","        self.left._prune(method, max_depth, min_criterion, n_samples)\n","        self.right._prune(method, max_depth, min_criterion, n_samples)\n","\n","        pruning = False\n","\n","        if method == 'impurity' and self.left.feature is None and self.right.feature is None: \n","            if (self.gain * float(self.n_samples) / n_samples) < min_criterion:\n","                pruning = True\n","        elif method == 'depth' and self.depth >= max_depth:\n","            pruning = True\n","\n","        if pruning is True:\n","            self.left = None\n","            self.right = None\n","            self.feature = None\n","\n","    def _predict(self, d):\n","        \"\"\"\"\n","        The internal function takes in a row of input values and recursively checks a threshold value until a\n","        predicted label is returned.\n","\n","        Parameters:\n","        d (series): [1 * p] Observed data samples of p attributes\n","\n","        Returns:\n","        string: Predicted label\n","        \"\"\"\n","        if self.feature != None:\n","            if d[self.feature] <= self.threshold:\n","                return self.left._predict(d)\n","            else:\n","                return self.right._predict(d)\n","        else:\n","            return self.label\n","\n","    def _show_tree(self, depth, cond):\n","        \"\"\"\n","        Internal helper function to print tree decision structure.\n","        \"\"\"\n","        base = '---' * depth + cond\n","        if self.feature != None:\n","            print(base + 'if ' + self.featurename + ' <= ' + f\"{self.threshold:.2f}\")\n","            self.left._show_tree(depth+1, 'then ')\n","            self.right._show_tree(depth+1, 'else ')\n","        else:\n","            print(base + '{class is: ' + str(self.label) + ', number of samples: ' + str(self.n_samples) + '}')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AHmsgU0uXV-"},"source":["### **Load Iris dataset from seaborn library and split into train and test sets**"]},{"cell_type":"code","metadata":{"id":"rChJwilFsHln","executionInfo":{"status":"ok","timestamp":1601960025436,"user_tz":-660,"elapsed":985,"user":{"displayName":"Patrick Songco","photoUrl":"","userId":"05048570303560894396"}}},"source":["iris = sns.load_dataset('iris')\n","X, y = iris.iloc[:, 0:4], iris['species']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84VfADzNu249"},"source":["### **Create instance of CART decision tree**\n","\n","*   Fit the training data and print CART Decision Tree\n","*   Make predictions on iris data test set and print accuracy\n","*   Compare accuracy to Sklearn Decision Tree Classifier\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uTnHohfwt8cb","executionInfo":{"status":"ok","timestamp":1601960033409,"user_tz":-660,"elapsed":1684,"user":{"displayName":"Patrick Songco","photoUrl":"","userId":"05048570303560894396"}},"outputId":"76633f9b-8565-4bda-a2bf-78dbaf558a75","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["print('\\nCART Decision Tree\\n-------------------------------------------------------------------')\n","cls = CART_decision_tree(tree = 'cls', criterion = 'entropy', prune = 'depth', max_depth = 3)\n","cls.fit(X_train, y_train)\n","cls.print_tree()\n","\n","pred = cls.predict(X_test)\n","print(\"\\nCART Decision Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))\n","\n","clf = sktree.DecisionTreeClassifier(criterion = 'entropy')\n","clf = clf.fit(X_train, y_train)\n","sk_pred = clf.predict(X_test)\n","print(\"Sklearn Library Tree Prediction Accuracy:  {}\".format(sum(sk_pred == y_test) / len(pred)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","CART Decision Tree\n","-------------------------------------------------------------------\n","if petal_length <= 2.50\n","---then {class is: setosa, number of samples: 35}\n","---else if petal_length <= 4.70\n","------then if petal_width <= 1.55\n","---------then {class is: versicolor, number of samples: 33}\n","---------else {class is: versicolor, number of samples: 2}\n","------else if petal_length <= 5.10\n","---------then {class is: virginica, number of samples: 16}\n","---------else {class is: virginica, number of samples: 26}\n","\n","CART Decision Tree Prediction Accuracy:    0.9736842105263158\n","Sklearn Library Tree Prediction Accuracy:  0.9736842105263158\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a_H3oxeX55Ax"},"source":[""],"execution_count":null,"outputs":[]}]}